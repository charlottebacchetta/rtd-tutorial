{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 - Leading causes of death\n",
    "### Charlotte Bacchetta\n",
    "### Date: 2024-11-08\n",
    "\n",
    "\n",
    "We will:\n",
    "- Load and explore a new dataset\n",
    "- Calculate mean, median, and mode using both `pandas` and the standard library\n",
    "- Visualize the data using basic Python output techniques\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = \"vscode+jupyterlab+notebook_connected\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the Dataset\n",
    "\n",
    "Link to the dataset : https://data.cityofnewyork.us/Health/New-York-City-Leading-Causes-of-Death/jb7j-dtam/about_data\n",
    "\n",
    "The dataset used in this project should contain at least one numeric column. We will start by loading the dataset and inspecting its contents.\n",
    "\n",
    "_Instructions:_ Update the `file_path` variable with the path to your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Leading Cause</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Race Ethnicity</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Death Rate</th>\n",
       "      <th>Age Adjusted Death Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011</td>\n",
       "      <td>Nephritis, Nephrotic Syndrome and Nephrisis (N...</td>\n",
       "      <td>F</td>\n",
       "      <td>Black Non-Hispanic</td>\n",
       "      <td>83</td>\n",
       "      <td>7.9</td>\n",
       "      <td>6.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009</td>\n",
       "      <td>Human Immunodeficiency Virus Disease (HIV: B20...</td>\n",
       "      <td>F</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>96</td>\n",
       "      <td>8</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009</td>\n",
       "      <td>Chronic Lower Respiratory Diseases (J40-J47)</td>\n",
       "      <td>F</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>155</td>\n",
       "      <td>12.9</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008</td>\n",
       "      <td>Diseases of Heart (I00-I09, I11, I13, I20-I51)</td>\n",
       "      <td>F</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>1445</td>\n",
       "      <td>122.3</td>\n",
       "      <td>160.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009</td>\n",
       "      <td>Alzheimer's Disease (G30)</td>\n",
       "      <td>F</td>\n",
       "      <td>Asian and Pacific Islander</td>\n",
       "      <td>14</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year                                      Leading Cause Sex  \\\n",
       "0  2011  Nephritis, Nephrotic Syndrome and Nephrisis (N...   F   \n",
       "1  2009  Human Immunodeficiency Virus Disease (HIV: B20...   F   \n",
       "2  2009       Chronic Lower Respiratory Diseases (J40-J47)   F   \n",
       "3  2008     Diseases of Heart (I00-I09, I11, I13, I20-I51)   F   \n",
       "4  2009                          Alzheimer's Disease (G30)   F   \n",
       "\n",
       "               Race Ethnicity Deaths Death Rate Age Adjusted Death Rate  \n",
       "0          Black Non-Hispanic     83        7.9                     6.9  \n",
       "1                    Hispanic     96          8                     8.1  \n",
       "2                    Hispanic    155       12.9                      16  \n",
       "3                    Hispanic   1445      122.3                   160.7  \n",
       "4  Asian and Pacific Islander     14        2.5                     3.6  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Information before conversion:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2102 entries, 0 to 2101\n",
      "Data columns (total 7 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   Year                     2102 non-null   int64 \n",
      " 1   Leading Cause            2102 non-null   object\n",
      " 2   Sex                      2102 non-null   object\n",
      " 3   Race Ethnicity           2102 non-null   object\n",
      " 4   Deaths                   2102 non-null   object\n",
      " 5   Death Rate               1759 non-null   object\n",
      " 6   Age Adjusted Death Rate  1759 non-null   object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 115.1+ KB\n",
      "\n",
      "Missing values per column before conversion:\n",
      "Year                         0\n",
      "Leading Cause                0\n",
      "Sex                          0\n",
      "Race Ethnicity               0\n",
      "Deaths                       0\n",
      "Death Rate                 343\n",
      "Age Adjusted Death Rate    343\n",
      "dtype: int64\n",
      "\n",
      "Dataset Information after 'Deaths' column conversion:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1964 entries, 0 to 2101\n",
      "Data columns (total 7 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Year                     1964 non-null   int64  \n",
      " 1   Leading Cause            1964 non-null   object \n",
      " 2   Sex                      1964 non-null   object \n",
      " 3   Race Ethnicity           1964 non-null   object \n",
      " 4   Deaths                   1964 non-null   float64\n",
      " 5   Death Rate               1621 non-null   object \n",
      " 6   Age Adjusted Death Rate  1621 non-null   object \n",
      "dtypes: float64(1), int64(1), object(5)\n",
      "memory usage: 122.8+ KB\n",
      "\n",
      "Numeric columns: ['Year', 'Deaths']\n",
      "\n",
      "Number of rows after cleanup: 1964\n",
      "The dataset meets the row count criteria (1,000 to 1,000,000 rows).\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import Libraries and Load Dataset\n",
    "# In this step, we load, explore, and prepare the dataset to ensure it meets project criteria.\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file path for the dataset\n",
    "file_path = '/Users/charlottebacchetta/Desktop/Columbia/Computing in Context - U6006/Project 1/New_York_City_Leading_Causes_of_Death_20241108.csv'\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to get an initial look at the data\n",
    "print(\"First few rows of the dataset:\")\n",
    "display(data.head())\n",
    "\n",
    "# Display basic information about the dataset, including column names, data types, and number of entries\n",
    "print(\"\\nDataset Information before conversion:\")\n",
    "data.info()\n",
    "\n",
    "# Check for missing values in each column to assess data quality\n",
    "print(\"\\nMissing values per column before conversion:\")\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "# Convert 'Deaths' column to numeric, forcing non-numeric values to NaN\n",
    "# This step is necessary to ensure we have clean numeric data for analysis\n",
    "data['Deaths'] = pd.to_numeric(data['Deaths'], errors='coerce')\n",
    "\n",
    "# Drop rows where 'Deaths' is NaN after conversion to keep only valid numeric data\n",
    "data = data.dropna(subset=['Deaths'])\n",
    "\n",
    "# Display updated information about the dataset after conversion\n",
    "print(\"\\nDataset Information after 'Deaths' column conversion:\")\n",
    "data.info()\n",
    "\n",
    "# Verify that the dataset meets project requirements:\n",
    "# - At least one numeric column (confirmed with 'Deaths')\n",
    "# - Between 1,000 and 1,000,000 rows\n",
    "\n",
    "# Check for numeric columns (after conversion)\n",
    "numeric_columns = data.select_dtypes(include=['number']).columns.tolist()\n",
    "print(\"\\nNumeric columns:\", numeric_columns)\n",
    "\n",
    "# Check the number of rows to confirm dataset size\n",
    "num_rows = data.shape[0]\n",
    "print(f\"\\nNumber of rows after cleanup: {num_rows}\")\n",
    "\n",
    "# Confirm if the dataset meets the row criteria\n",
    "if num_rows >= 1000 and num_rows <= 1000000:\n",
    "    print(\"The dataset meets the row count criteria (1,000 to 1,000,000 rows).\")\n",
    "else:\n",
    "    print(\"The dataset does NOT meet the row count criteria.\")\n",
    "\n",
    "# Explanation:\n",
    "# - We converted the 'Deaths' column to numeric to ensure data consistency.\n",
    "# - `errors='coerce'` converts non-numeric values in 'Deaths' to NaN, which we then drop.\n",
    "# - This prepares the dataset for accurate analysis in subsequent steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Identify a Numeric Column\n",
    "We'll select a numeric column from the dataset to analyze. This column will be used to calculate summary statistics.\n",
    "\n",
    "_Instructions:_ Inspect the dataset above and identify an appropriate numeric column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'Deaths' column is confirmed to be numeric.\n",
      "\n",
      "Basic statistics for the 'Deaths' column:\n",
      "count    1964.000000\n",
      "mean      429.256110\n",
      "std       827.583725\n",
      "min         1.000000\n",
      "25%        25.000000\n",
      "50%       140.000000\n",
      "75%       317.250000\n",
      "max      7050.000000\n",
      "Name: Deaths, dtype: float64\n",
      "\n",
      "Sample entries in 'Deaths' column:\n",
      "0      83.0\n",
      "1      96.0\n",
      "2     155.0\n",
      "3    1445.0\n",
      "4      14.0\n",
      "Name: Deaths, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Focus on the 'Deaths' Column for Analysis\n",
    "# In this step, we isolate the 'Deaths' column, which we prepared in Step 1 as a clean numeric column.\n",
    "\n",
    "# Isolate the 'Deaths' column for statistical analysis\n",
    "deaths_column = 'Deaths'\n",
    "data_deaths = data[deaths_column]  # 'data' is already filtered for NaN values in 'Deaths'\n",
    "\n",
    "# Verify that 'Deaths' is numeric for statistical analysis\n",
    "if pd.api.types.is_numeric_dtype(data_deaths):\n",
    "    print(f\"The '{deaths_column}' column is confirmed to be numeric.\\n\")\n",
    "else:\n",
    "    print(f\"Warning: The '{deaths_column}' column is not numeric. Check data format.\\n\")\n",
    "\n",
    "# Display basic statistics to understand the distribution of death counts\n",
    "print(\"Basic statistics for the 'Deaths' column:\")\n",
    "print(data_deaths.describe())\n",
    "\n",
    "# Display a few entries to confirm data integrity\n",
    "print(\"\\nSample entries in 'Deaths' column:\")\n",
    "print(data_deaths.head(5))\n",
    "\n",
    "# Explanation:\n",
    "# - We focus on 'Deaths' as it is a key numeric column, necessary for statistical calculations.\n",
    "# - This column has been cleaned for non-numeric and missing values, ensuring reliability in our analysis.\n",
    "# - The `describe()` method provides a summary of the distribution, including mean, min, max, and quartiles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calculate Mean, Median, and Mode Using pandas\n",
    "Now that we have our numeric data, we will calculate the mean, median, and mode using `pandas`.\n",
    "\n",
    "_Instructions:_ Run the code below to compute these statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for 'Deaths' column using pandas:\n",
      "Mean (pandas): 429.2561099796334\n",
      "Median (pandas): 140.0\n",
      "Mode (pandas): 1.0\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Calculate Mean, Median, and Mode Using pandas\n",
    "# In this step, we use pandas to calculate key statistics for the 'Deaths' column: mean, median, and mode.\n",
    "\n",
    "# Calculate the mean of the 'Deaths' column to understand the average annual deaths\n",
    "mean_pandas = data_deaths.mean()\n",
    "\n",
    "# Calculate the median of the 'Deaths' column to find the middle value in the dataset\n",
    "median_pandas = data_deaths.median()\n",
    "\n",
    "# Calculate the mode of the 'Deaths' column to identify the most frequently occurring death count\n",
    "# The mode() function may return multiple values; we select the first mode if there are multiple\n",
    "mode_pandas = data_deaths.mode().iloc[0]\n",
    "\n",
    "# Display the calculated statistics\n",
    "print(\"Statistics for 'Deaths' column using pandas:\")\n",
    "print(f\"Mean (pandas): {mean_pandas}\")\n",
    "print(f\"Median (pandas): {median_pandas}\")\n",
    "print(f\"Mode (pandas): {mode_pandas}\")\n",
    "\n",
    "# Explanation:\n",
    "# - The mean provides the average death count, giving a sense of the central tendency.\n",
    "# - The median identifies the midpoint of the data, which can help understand the distribution.\n",
    "# - The mode reveals the most common death count, indicating any significant frequency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Calculate Mean, Median, and Mode Using Standard Python Library\n",
    "To reinforce understanding, we will repeat these calculations without using `pandas`.\n",
    "\n",
    "_Instructions:_ Use basic Python functions to calculate these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for 'Deaths' column using standard Python functions:\n",
      "Mean (standard library): 429.2561099796334\n",
      "Median (standard library): 140.0\n",
      "Mode (standard library): 1.0\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Calculate Mean, Median, and Mode Using the Standard Library\n",
    "# In this step, we calculate the mean, median, and mode of the 'Deaths' data using only standard Python functions.\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Convert data_deaths to a list to ensure compatibility with standard Python functions\n",
    "data_list = data_deaths.tolist()\n",
    "\n",
    "# Mean calculation\n",
    "# The mean is calculated as the sum of all values divided by the number of entries\n",
    "mean_stdlib = sum(data_list) / len(data_list)\n",
    "\n",
    "# Median calculation\n",
    "# To find the median, we first sort the data and then select the middle value (or average of two middle values if even length)\n",
    "sorted_data = sorted(data_list)\n",
    "n = len(sorted_data)\n",
    "median_stdlib = sorted_data[n // 2] if n % 2 != 0 else (sorted_data[n // 2 - 1] + sorted_data[n // 2]) / 2\n",
    "\n",
    "# Mode calculation\n",
    "# Using Counter, we count occurrences of each value and select the most frequent one as the mode\n",
    "data_counts = Counter(data_list)\n",
    "mode_stdlib = max(data_counts, key=data_counts.get)\n",
    "\n",
    "# Display the results for mean, median, and mode\n",
    "print(\"Statistics for 'Deaths' column using standard Python functions:\")\n",
    "print(f\"Mean (standard library): {mean_stdlib}\")\n",
    "print(f\"Median (standard library): {median_stdlib}\")\n",
    "print(f\"Mode (standard library): {mode_stdlib}\")\n",
    "\n",
    "# Explanation:\n",
    "# - The mean provides the average death count, calculated manually without pandas.\n",
    "# - The median is the midpoint of sorted data, which helps indicate central tendency.\n",
    "# - The mode shows the most common death count, calculated using the Counter class.\n",
    "# - This exercise reinforces fundamental Python skills for basic statistical analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Data Visualization\n",
    "We will create a simple vertical bar chart using text. This visualization represents values in the numeric column selected earlier.\n",
    "\n",
    "_Instructions:_ Run the code below to view a text-based vertical bar chart of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Visualization (Total Deaths by Year):\n",
      "\n",
      "2007: ████████████████████████████████ (53996.0)\n",
      "2008: ████████████████████████████████ (54138.0)\n",
      "2009: ████████████████████████████████ (52820.0)\n",
      "2010: ███████████████████████████████ (52505.0)\n",
      "2011: ████████████████████████████████ (52726.0)\n",
      "2012: ███████████████████████████████ (52420.0)\n",
      "2013: ████████████████████████████████ (53387.0)\n",
      "2014: ████████████████████████████████ (53006.0)\n",
      "2015: ████████████████████████████████ (54120.0)\n",
      "2016: █████████████████████████████████ (54280.0)\n",
      "2017: █████████████████████████████████ (54319.0)\n",
      "2018: █████████████████████████████████ (55081.0)\n",
      "2019: █████████████████████████████████ (54559.0)\n",
      "2020: ██████████████████████████████████████████████████ (82142.0)\n",
      "2021: ██████████████████████████████████████ (63560.0)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Data Visualization - Text-Based Bar Chart for Total Deaths by Year\n",
    "# In this step, we create a simple text-based bar chart to represent total deaths for each year.\n",
    "# This visualization uses only standard Python functions and is designed to scale dynamically with the data.\n",
    "\n",
    "# Group the data by 'Year' and calculate the total deaths for each year\n",
    "deaths_by_year = data.groupby('Year')['Deaths'].sum()\n",
    "\n",
    "# Convert the grouped data to lists for easier handling in text-based visualization\n",
    "years = deaths_by_year.index.tolist()\n",
    "totals = deaths_by_year.tolist()\n",
    "\n",
    "# Title for the visualization\n",
    "print(\"Data Visualization (Total Deaths by Year):\\n\")\n",
    "\n",
    "# Determine the maximum value in totals to set a scaling factor for the bar width\n",
    "# The scaling factor allows us to fit bars within a 50-character width for readability on narrow screens\n",
    "max_value = max(totals)\n",
    "scale_factor = 50 / max_value  # Adjust scaling based on the maximum value to fit within 50 characters\n",
    "\n",
    "# Generate and display the bar chart, where each bar's length represents the total deaths for that year\n",
    "for year, total in zip(years, totals):\n",
    "    # Create a bar by repeating the block character proportional to the scaled total deaths\n",
    "    bar = \"█\" * int(total * scale_factor)\n",
    "    print(f\"{year}: {bar} ({total})\")\n",
    "\n",
    "# Explanation:\n",
    "# - This text-based bar chart provides a simple way to visualize total deaths by year.\n",
    "# - The length of each bar is scaled to fit within a maximum width, making it adaptable to different screen sizes.\n",
    "# - Each line represents a year, with the bar length indicating the total deaths for that year.\n",
    "# - This approach meets the project requirements by using only the standard library and creating a dynamic visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
